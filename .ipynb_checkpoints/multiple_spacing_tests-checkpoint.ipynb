{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQLUUPEzngoM"
   },
   "source": [
    "# GtSt: Multiple Testing and Variable Selection along Least Angle Regression's path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents the numerical experiments of the paper entitled \n",
    "\n",
    "> *Multiple Testing and Variable Selection along Least Angle Regression's path*, [arXiv:1906.12072v3](https://arxiv.org/abs/1906.12072v3).\n",
    "\n",
    "We present the following points:\n",
    "- A comparison of the **power and FDR control** on simulated data for **GtST, FCD and Knockoff** in **Section I**;\n",
    "- Presentation of **HIV dataset** in **Section II**;\n",
    "- A comparison of the **power and FDR control on HIV dataset** for **GtST, FCD, Knockoff and Slope** in **Section III**;\n",
    "- A **new formulation of LARS algorithm** in **Section IV**;\n",
    "\n",
    "The methods considered are:\n",
    "- **[Knockoff]** Knockoff filters for FDR control and we use the implementation presented on the webpage <https://web.stanford.edu/group/candes/knockoffs/> based on the paper:\n",
    "> *Controlling the False Discovery Rate via Knockoffs*, [arXiv:1404.5609](https://arxiv.org/abs/1404.5609);\n",
    "- **[FCD]** False Discovery Control via Debiasing and we use the implementation of debiased lasso presented on the webpage <https://web.stanford.edu/~montanar/sslasso/> with the theoretical value $\\bar\\lambda=2\\sqrt{(2\\log p)/n}$ for the regularizing parameter as presented in the paper:\n",
    "> *False Discovery Rate Control via Debiased Lasso*, [arXiv:1803.04464](https://arxiv.org/abs/1803.04464);\n",
    "- **[Slope]** Slope for FDR control, as presented in the paper:\n",
    "> *SLOPE - Adaptive variable selection via convex optimization* [arXiv:1407.3824](https://arxiv.org/abs/1407.3824);\n",
    "- **[GtSt-BH]** Generalized t-Spacing tests on successive entries of the LARS path comibined with a Benjaminiâ€“Hochberg procedure based on the sequence of spacings $\\hat  \\beta_{012},\\hat  \\beta_{123},\\ldots,\\hat  \\beta_{a(a+1)(a+2)},\\ldots$ with nominal value $\\alpha= 0.1$ and presented in the paper:\n",
    "> *Multiple Testing and Variable Selection along Least Angle Regression's path*, [arXiv:1906.12072v3](https://arxiv.org/abs/1906.12072v3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the following packages for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from lars.multiple_spacing_tests import generate_data\n",
    "from lars.LARS import LARS\n",
    "from lars.FDR import Testing\n",
    "from lars.FCD import FCD\n",
    "from sklearn import linear_model\n",
    "import scipy as sc\n",
    "import cvxpy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. FDR and power on simulated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the experiments introduced in Section 5 of the paper:\n",
    "> *False Discovery Rate Control via Debiased Lasso*, [arXiv:1803.04464](https://arxiv.org/abs/1803.04464);\n",
    "\n",
    "As in this reference, we consider the linear model with a design $X$ with independent rows draw with respect to $\\mathcal N_p(0,\\Sigma)$. The covariance $\\Sigma\\in\\mathbb R^{p\\times p}$ is such that $\\Sigma_{ij} = r^{|i-j|}$, for some parameter $r\\in(0,1)$. We then normalize the columns of~$X$ to have unit Euclidean norm. We draw a $s_0$-sparse vector $\\beta^0\\in \\mathbb R^p$ by choosing a support of size $s_0$ at random with values $\\{\\pm A\\}$ uniformly at random, where $A>0$ denotes the absolute value of the amplitudes. The Gaussian noise term $\\eta$ is drawn from $\\mathcal N_n(0,\\mathrm{Id}_{n\\times n})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ia. Effect of signal amplitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose $n = 100$, $p = 150$, $k = 10$, $\\eta = 0.1$ and vary the signal amplitude in the set $A\\in \\{0.5, 1, 1.5, \\dotsc, 5.5, 6\\}$. We compute the FDR and power by averaging across `nbite = 3000` realizations of noise and the generation of coefficient vector $\\beta^0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EFFECT AMPLITUDE\n",
    "# We choose the parameters as in the paper\n",
    "n, p, k, eta = 200, 100, 20, 0.1\n",
    "# we ran 10 Monte Carlo simulations for sake of time limitation (in the paper nbite = 3000)\n",
    "nbite = 10 #replace by 3000 to get the results presented in the paper arXiv:1906.12072v3\n",
    "\n",
    "list_A = np.arange(1,30,1)\n",
    "algos = ['FCD','Knockoff','GtSt-BH']\n",
    "\n",
    "FDR = {name:np.zeros(len(list_A)) for name in algos}\n",
    "power = {name:np.zeros(len(list_A)) for name in algos}\n",
    "\n",
    "for i, A in enumerate(list_A):\n",
    "    theta0, X, S, y = generate_data(n, p, k, eta, A)\n",
    "    \n",
    "    for _ in range(nbite):\n",
    "        theta0, X, S, y = generate_data(n, p, k, eta, A)\n",
    "        model = Testing()\n",
    "        signhat, Shat = FCD(n, p, X, y)\n",
    "        FDR['FCD'][i] += model.FDR(Shat, S) / nbite\n",
    "        power['FCD'][i] += model.power(Shat, S) / nbite\n",
    "        fdr, pow = model.fdr_power_lars(X,y,S,0.1)\n",
    "        FDR['GtSt-BH'][i] += fdr / nbite\n",
    "        power['GtSt-BH'][i] += pow / nbite\n",
    "        fdr, pow = model.fdr_power_knockoffs(X,y,S,alpha=0.1)#,mode='SDP')\n",
    "        FDR['Knockoff'][i] += fdr / nbite\n",
    "        power['Knockoff'][i] += pow / nbite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs('figuresSimulated')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(list_A, 100*FDR['FCD'], label='FCD')\n",
    "plt.plot(list_A, 100*FDR['GtSt-BH'], label='GtSt-BH')\n",
    "plt.plot(list_A, 100*FDR['Knockoff'], label='Knockoff')\n",
    "plt.xlabel('Amplitude ($A$)')\n",
    "plt.ylabel('FDR')\n",
    "plt.hlines(10, min(list_A), max(list_A), colors='black', linestyle='--', label='nominal level')\n",
    "plt.legend()\n",
    "plt.savefig('figuresSimulated/amplitude_effect_fdr.png',dpi=300)\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(list_A, 100*power['FCD'], label='FCD')\n",
    "plt.plot(list_A, 100*power['GtSt-BH'], label='GtSt-BH')\n",
    "plt.plot(list_A, 100*power['Knockoff'], label='Knockoff')\n",
    "plt.xlabel('Amplitude ($A$)')\n",
    "plt.ylabel('Power')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('figuresSimulated/amplitude_effect_power.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ib. Effect of feature correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the effect of feature correlations. We set $n =100$, $p = 150$, $k = 10$, $A = 8$. Recall that the rows of the design matrix $X$ are generated from a $\\mathcal N_p(0,\\Sigma)$ distribution, with $\\Sigma_{ij} = \\eta^{|i-j|}$, and then the columns of $X$ are normalized to have unit norm. We vary the  parameter $\\eta$ in the set $\\{0.1, 0.15, 0.2, \\dotsc, 0.75, 0.8\\}$. For each value of $\\eta$, we compute FDR and power for both methods, by averaging over `nbite = 3000` realizations of noise and design matrix $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EFFECT CORRELATION\n",
    "# We choose the parameters as in the paper\n",
    "n, p, k, A = 200, 150, 20, 10\n",
    "# we ran 10 Monte Carlo simulations for sake of time limitation (in the paper nbite = 1000)\n",
    "nbite = 10 #replace by 3000 to get the results presented in the paper arXiv:1906.12072v3\n",
    "\n",
    "list_eta = np.arange(0.1,0.95,0.1)\n",
    "\n",
    "algos = ['FCD','Knockoff','GtSt-BH']\n",
    "\n",
    "FDR = {name:np.zeros(len(list_eta)) for name in algos}\n",
    "power = {name:np.zeros(len(list_eta)) for name in algos}\n",
    "\n",
    "for i, eta in enumerate(list_eta):\n",
    "    theta0, X, S, y = generate_data(n, p, k, eta, A)\n",
    "\n",
    "    for _ in range(nbite):\n",
    "        theta0, X, S, y = generate_data(n, p, k, eta, A)\n",
    "        model = Testing()\n",
    "        signhat, Shat = FCD(n, p, X, y)\n",
    "        FDR['FCD'][i] += model.FDR(Shat, S) / nbite\n",
    "        power['FCD'][i] += model.power(Shat, S) / nbite\n",
    "        \n",
    "        fdr, pow = model.fdr_power_lars(X,y,S,0.1)\n",
    "        FDR['GtSt-BH'][i] += fdr / nbite\n",
    "        power['GtSt-BH'][i] += pow / nbite\n",
    "\n",
    "        fdr, pow = model.fdr_power_knockoffs(X,y,S,alpha=0.1)\n",
    "        FDR['Knockoff'][i] += fdr / nbite\n",
    "        power['Knockoff'][i] += pow / nbite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(3)\n",
    "plt.plot(list_eta, 100*FDR['FCD'], label='FCD')\n",
    "plt.plot(list_eta, 100*FDR['GtSt-BH'], label='GtSt-BH')\n",
    "plt.plot(list_eta, 100*FDR['Knockoff'], label='Knockoff')\n",
    "plt.xlabel('Feature Correlation ($\\eta$)')\n",
    "plt.ylabel('FDR')\n",
    "plt.hlines(10, min(list_eta), max(list_eta), colors='black', linestyle='--', label='nominal level')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('figuresSimulated/corr_effect_fdr.png',dpi=300)\n",
    "\n",
    "plt.figure(4)\n",
    "plt.plot(list_eta, 100*power['FCD'], label='FCD')\n",
    "plt.plot(list_eta, 100*power['GtSt-BH'], label='GtSt-BH')\n",
    "plt.plot(list_eta, 100*power['Knockoff'], label='Knockoff')\n",
    "plt.xlabel('Feature Correlation ($\\eta$)')\n",
    "plt.ylabel('Power')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('figuresSimulated/corr_effect_power.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ic. Effect of sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set $n = 100$, $p = 150$, $A = 8$, $\\eta = 0.1$ and vary the sparsity level of the coefficients in the set $k\\in\\{10,15, 20, \\dotsc, 130\\}$. For both methods, the power and FDR are computed by averaging over `nbite = 3000` trials of noise and the generation of coefficient vector $\\beta^0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EFFECT of SPARSITY\n",
    "# We choose the parameters as in the paper\n",
    "n, p, eta, A = 200, 150, 0.1, 10\n",
    "# we ran 10 Monte Carlo simulations for sake of time limitation (in the paper nbite = 1000)\n",
    "nbite = 10 #replace by 3000 to get the results presented in the paper arXiv:1906.12072v3\n",
    "\n",
    "list_k = np.arange(1,40,10)\n",
    "\n",
    "algos = ['FCD','Knockoff','GtSt-BH']\n",
    "\n",
    "FDR = {name:np.zeros(len(list_k)) for name in algos}\n",
    "power = {name:np.zeros(len(list_k)) for name in algos}\n",
    "\n",
    "for i, k in enumerate(list_k):\n",
    "    theta0, X, S, y = generate_data(n, p, k, eta, A)\n",
    "\n",
    "    for _ in range(nbite):\n",
    "        theta0, X, S, y = generate_data(n, p, k, eta, A)\n",
    "        model = Testing()\n",
    "        signhat, Shat = FCD(n, p, X, y)\n",
    "        FDR['FCD'][i] += model.FDR(Shat, S) / nbite\n",
    "        power['FCD'][i] += model.power(Shat, S) / nbite\n",
    "        \n",
    "        fdr, pow = model.fdr_power_lars(X,y,S,0.1)\n",
    "        FDR['GtSt-BH'][i] += fdr / nbite\n",
    "        power['GtSt-BH'][i] += pow / nbite\n",
    "\n",
    "        fdr, pow = model.fdr_power_knockoffs(X,y,S,alpha=0.1)#,mode='SDP')\n",
    "        FDR['Knockoff'][i] += fdr / nbite\n",
    "        power['Knockoff'][i] += pow / nbite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(5)\n",
    "plt.plot(list_k, 100*FDR['FCD'], label='FCD')\n",
    "plt.plot(list_k, 100*FDR['GtSt-BH'], label='GtSt-BH')\n",
    "plt.plot(list_k, 100*FDR['Knockoff'], label='Knockoff')\n",
    "plt.xlabel('Sparsity Level ($k$)')\n",
    "plt.ylabel('FDR')\n",
    "plt.hlines(10, min(list_k), max(list_k), colors='black', linestyle='--', label='nominal level')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('figuresSimulated/sparsity_effect_fdr.png',dpi=300)\n",
    "\n",
    "plt.figure(6)\n",
    "plt.plot(list_k, 100*power['FCD'], label='FCD')\n",
    "plt.plot(list_k, 100*power['GtSt-BH'], label='GtSt-BH')\n",
    "plt.plot(list_k, 100*power['Knockoff'], label='Knockoff')\n",
    "plt.xlabel('Sparsity Level ($k$)')\n",
    "plt.ylabel('Power')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('figuresSimulated/sparsity_effect_power.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Analysis of HIV dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scientific goal is to determine which mutations of the Human Immunodeficiency Virus Type 1 (HIV-1) are associated with drug resistance. The data set, publicly available from the [Stanford HIV Drug Resistance Database](https://hivdb.stanford.edu/pages/published_analysis/genophenoPNAS2006/), was originally analyzed in (Rhee et al. 2006). The analysis described here is based on the article \n",
    "> *Multiple Testing and Variable Selection along Least Angle Regression's path*, [arXiv:1906.12072v3](https://arxiv.org/abs/1906.12072v3).\n",
    "\n",
    "The dataset has been prepared as in \n",
    "\n",
    "> *Controlling the False Discovery Rate via Knockoffs*, [arXiv:1404.5609](https://arxiv.org/abs/1404.5609).\n",
    "\n",
    "We present here the selected proatease positions that may present a resistance to APV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyreadr\n",
    "import pyreadr\n",
    "\n",
    "def get_data(name):\n",
    "    '''\n",
    "    Load HIV data. The fils have been created using the preprocessing presented in \n",
    "    https://cran.r-project.org/web/packages/knockoff/vignettes/hiv.html\n",
    "    '''\n",
    "    result = pyreadr.read_r('data/y'+name+'.RData')\n",
    "    Y = result['y']\n",
    "    y = np.array(Y).reshape(-1)\n",
    "    X = pd.read_csv(\"data/X\"+name+\".csv\", header=0,index_col=0)\n",
    "    n,p = X.shape\n",
    "    X = np.array(X).reshape(n,p)\n",
    "    result = pyreadr.read_r('data/labels'+name+'.RData')\n",
    "    labels = result['labels']\n",
    "    labels = np.array(labels).reshape(-1)\n",
    "    return y, X, labels\n",
    "\n",
    "def stacked_bar(data, series_labels, category_labels=None,\n",
    "\t\t\t\t\tshow_values=False, value_format=\"{}\", y_label=None,\n",
    "\t\t\t\t\tgrid=True, reverse=False):\n",
    "\t\t\"\"\"Plots a stacked bar chart with the data and labels provided.\n",
    "\n",
    "\t\tKeyword arguments:\n",
    "\t\tdata\t\t\t-- 2-dimensional numpy array or nested list\n",
    "\t\t\t\t\t\t   containing data for each series in rows\n",
    "\t\tseries_labels   -- list of series labels (these appear in\n",
    "\t\t\t\t\t\t   the legend)\n",
    "\t\tcategory_labels -- list of category labels (these appear\n",
    "\t\t\t\t\t\t   on the x-axis)\n",
    "\t\tshow_values\t -- If True then numeric value labels will \n",
    "\t\t\t\t\t\t   be shown on each bar\n",
    "\t\tvalue_format\t-- Format string for numeric value labels\n",
    "\t\t\t\t\t\t   (default is \"{}\")\n",
    "\t\ty_label\t\t -- Label for y-axis (str)\n",
    "\t\tgrid\t\t\t-- If True display grid\n",
    "\t\treverse\t\t -- If True reverse the order that the\n",
    "\t\t\t\t\t\t   series are displayed (left-to-right\n",
    "\t\t\t\t\t\t   or right-to-left)\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tny = len(data[0])\n",
    "\t\tind = list(range(ny))\n",
    "\n",
    "\t\taxes = []\n",
    "\t\tcum_size = np.zeros(ny)\n",
    "\n",
    "\t\tdata = np.array(data)\n",
    "\n",
    "\t\tif reverse:\n",
    "\t\t\tdata = np.flip(data, axis=1)\n",
    "\t\t\tcategory_labels = reversed(category_labels)\n",
    "\n",
    "\t\tfor i, row_data in enumerate(data):\n",
    "\t\t\taxes.append(plt.bar(ind, row_data, bottom=cum_size,\n",
    "\t\t\t\t\t\t\t\tlabel=series_labels[i]))\n",
    "\t\t\tcum_size += row_data\n",
    "\n",
    "\t\tif category_labels:\n",
    "\t\t\tplt.xticks(ind, category_labels)\n",
    "\n",
    "\t\tif y_label:\n",
    "\t\t\tplt.ylabel(y_label)\n",
    "\n",
    "\t\tplt.legend()\n",
    "\n",
    "\t\tif grid:\n",
    "\t\t\tplt.grid()\n",
    "\n",
    "\t\tif show_values:\n",
    "\t\t\tfor axis in axes:\n",
    "\t\t\t\tfor bar in axis:\n",
    "\t\t\t\t\tw, h = bar.get_width(), bar.get_height()\n",
    "\t\t\t\t\tplt.text(bar.get_x() + w / 2, bar.get_y() + h / 2,\n",
    "\t\t\t\t\t\t\t int(h), ha=\"center\",\n",
    "\t\t\t\t\t\t\t va=\"center\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Comparison of GtSt, FCD, Knockoff and Slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next lines give the result of the `methods = ['lars','KSDP', 'KEQUI','FCD','SLOPE']` for the HIV drug resistance dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "FrZRFJm8kAsF",
    "outputId": "c084b5ea-82e1-4171-acf5-d9499ffb1b1c"
   },
   "outputs": [],
   "source": [
    "datasets = ['APV', 'NFV', 'IDV','ATV', 'LPV', 'RTV','SQV']\n",
    "result = pyreadr.read_r('data/truelabels.RData')\n",
    "true_vars = result['truelabels']\n",
    "true_vars = np.array(true_vars).reshape(-1)\n",
    "methods = ['lars','KSDP', 'KEQUI','FCD','SLOPE']\n",
    "IN = [np.zeros(len(datasets)) for name in methods]\n",
    "OUT = [np.zeros(len(datasets)) for name in methods]\n",
    "\n",
    "import re\n",
    "for i, namedata in enumerate(datasets):\n",
    "    y, X, labels = get_data(namedata)\n",
    "    # \n",
    "    support2var = ['' for i in labels]\n",
    "    for j,name in enumerate(labels):\n",
    "        support2var[j] = int(re.sub(\"[^0-9]\", \"\", name))\n",
    "    support2var = np.array(support2var)\n",
    "    model = Testing()\n",
    "    model.compute_lars_path(X, y, lars_algorithm='recursive', normalization=True)\n",
    "    for k,algo in enumerate(methods):\n",
    "        if algo=='FCD':\n",
    "            n, p = X.shape \n",
    "            signhat, Shat = FCD(n, p, X, y, level=0.2)\n",
    "            variables = support2var[Shat]\n",
    "            outs = len(set(variables)-set(true_vars))\n",
    "            ins = len(true_vars) - len(set(true_vars)-set(variables))\n",
    "            IN[k][i] = ins\n",
    "            OUT[k][i] = outs\n",
    "        elif algo=='SLOPE':\n",
    "            prediction = np.array(pd.read_csv(\"data/SLOPE/slopeHIV-\"+namedata+\".csv\", header=0,index_col=0)).reshape(-1)\n",
    "            prediction -= 1 # since R indexation starts at 1\n",
    "            variables = support2var[prediction]\n",
    "            outs = len(set(variables)-set(true_vars))\n",
    "            ins = len(true_vars) - len(set(true_vars)-set(variables))\n",
    "            IN[k][i] = ins\n",
    "            OUT[k][i] = outs\n",
    "        else:\n",
    "            IN[k][i], OUT[k][i] = model.in_out_support(true_vars, support2var, X=X, y=y, alpha=0.2, method=algo, K1=101)\n",
    "\n",
    "\n",
    "IN = np.array(IN)\n",
    "OUT = np.array(OUT)\n",
    "category_labels = ['GtSt-BH', 'Knockoff1', 'Knockoff2', 'FCD', 'SLOPE']\n",
    "series_labels = ['In TSM list', 'NOT in TSM list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,namedata in enumerate(datasets):\n",
    "    stacked_bar(np.array([IN[:,i],OUT[:,i]]), series_labels, category_labels=category_labels, show_values=True, value_format=\"{:d}\",y_label=\"HIV-1 protease positions selected\",grid=False,reverse=False)\n",
    "    plt.ylim([0,42])\n",
    "    plt.title('Resistance to '+namedata)\n",
    "    plt.savefig('figuresHIV/'+namedata+'.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. LARS reccursive formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the **APV dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X, labels = get_data('APV')# You can try: 'NFV', 'IDV','ATV', 'LPV', 'RTV','SQV'\n",
    "\n",
    "n,p = np.shape(X)\n",
    "print(\"There is p=%s observations for %s predictors.\" %(n,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the LAR's path using the *recursive formulation* of the article:\n",
    "> *Multiple Testing and Variable Selection along Least Angle Regression's path*, [arXiv:1906.12072v3](https://arxiv.org/abs/1906.12072v3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lars.multiple_spacing_tests import lar_rec\n",
    "\n",
    "print(\"Computing the recursive formulation of LAR as in (AzaÃ¯s and De Castro, 2021)\")\n",
    "\n",
    "lambdas, indexes, correls, R, Z = lar_rec(X, y, normalization = True) #Algorithm 1 in (AzaÃ¯s and De Castro, 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "print(\"Computing regularization path using the LAR from scikit learn.\")\n",
    "\n",
    "alphas, active, coefs = linear_model.lars_path(X, \n",
    "                                               y, \n",
    "                                               Xy           = Z[range(p)], \n",
    "                                               Gram         = R[0:p,0:p], \n",
    "                                               max_iter     = p-1, \n",
    "                                               method       = 'lar', \n",
    "                                               eps          = 1e-23,\n",
    "                                               verbose      = False, \n",
    "                                               return_path  = True)\n",
    "\n",
    "lambdas_lar_scikit_learn = n*alphas\n",
    "indexes_lar_scikit_learn = active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compare the two algorithms on the HIV dataset. They offer the same output except on the tail of the LAR's path where **`sklearn` gives abnormal values** (the sequence is no longer non-increasing as it should be)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "k_max = min(np.size(indexes_lar_scikit_learn), np.size(indexes))\n",
    "\n",
    "plt.plot(range(k_max-1),lambdas_lar_scikit_learn[:k_max-1])\n",
    "plt.plot(range(k_max-1),lambdas[:k_max-1])\n",
    "plt.legend([r'sklearn', r'recursive'])\n",
    "plt.tight_layout()\n",
    "plt.xlabel(r'$k$')\n",
    "plt.ylabel(r'$\\lambda_k$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 75 #we focus on the last 75 values\n",
    "plt.plot(range(k_max-d,k_max-1),lambdas_lar_scikit_learn[k_max-d:k_max-1])\n",
    "plt.plot(range(k_max-d,k_max-1),lambdas[k_max-d:k_max-1])\n",
    "plt.legend([r'sklearn', r'recursive'])\n",
    "plt.tight_layout()\n",
    "plt.xlabel(r'$k$')\n",
    "plt.ylabel(r'$\\lambda_k$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot, `SciKitLearn` returns incresing values which is prohibited. The two algorithm retun the same values on the $50$ first knots of LAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 50 #compare the 50 first knots\n",
    "plt.plot(lambdas_lar_scikit_learn[0:d])\n",
    "plt.plot(lambdas[0:d])\n",
    "plt.legend([r'sklearn', r'recursive'])\n",
    "plt.tight_layout()\n",
    "plt.xlabel(r'$k$')\n",
    "plt.ylabel(r'$\\lambda_k$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare the sequence of variables entering the model. Here again, they agree except on the tail of the LAR's path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agree = 1*np.array(indexes[:k_max-1] == indexes_lar_scikit_learn[:k_max-1])\n",
    "plt.stem(agree)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A deeper investigation shows that sklearn and the recuresive formulation differs at step $k=61$. Sklearn returns almost the same value $\\lambda$ at step $k=61$ and $k=62$, and incoherent values (namely an increasing $\\lambda$) at step $k=132$ (see below). We also computed the three others formualtions of the LAR (not showed here) and we found the same lambdas and indexes as in the recursive formulation presented here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 61\n",
    "end = 64\n",
    "lambdas_lar_scikit_learn[start:end], lambdas[start:end], lambdas_lar_scikit_learn[132:134]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 60\n",
    "end = 105\n",
    "plt.plot(range(start,end),lambdas_lar_scikit_learn[start:end])\n",
    "plt.plot(range(start, end),lambdas[start:end])\n",
    "plt.legend([r'sklearn', r'recursive'])\n",
    "plt.tight_layout()\n",
    "plt.xlabel(r'$k$')\n",
    "plt.ylabel(r'$\\lambda_k$')\n",
    "plt.show()\n",
    "\n",
    "start = 60\n",
    "end = 64\n",
    "plt.plot(range(start,end),lambdas_lar_scikit_learn[start:end])\n",
    "plt.plot(range(start, end),lambdas[start:end])\n",
    "plt.legend([r'sklearn', r'recursive'])\n",
    "plt.tight_layout()\n",
    "plt.xlabel(r'$k$')\n",
    "plt.ylabel(r'$\\lambda_k$')\n",
    "plt.show()\n",
    "\n",
    "start = 130\n",
    "end = 140\n",
    "plt.plot(range(start,end),lambdas_lar_scikit_learn[start:end])\n",
    "plt.plot(range(start, end),lambdas[start:end])\n",
    "plt.legend([r'sklearn', r'recursive'])\n",
    "plt.tight_layout()\n",
    "plt.xlabel(r'$k$')\n",
    "plt.ylabel(r'$\\lambda_k$')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HIV_experiments.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
